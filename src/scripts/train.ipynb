{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M18tABf-9v81"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Z_CtZecj8ugJ"
      },
      "outputs": [],
      "source": [
        "!unzip /content/drive/MyDrive/anchor_final.zip -d /content/\n",
        "!unzip /content/drive/MyDrive/train.zip -d /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "WTy3iRoD7_7C"
      },
      "outputs": [],
      "source": [
        "!unzip /content/drive/MyDrive/anchor_images.zip -d /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "q7hYnEPmZBAU"
      },
      "outputs": [],
      "source": [
        "!unzip /content/drive/MyDrive/test.zip -d /content/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evKSrUp_UA81"
      },
      "source": [
        "#0)  Load an Image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDOmfpBkT_2G",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Define the transformations including ColorJitter for brightness and contrast\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ColorJitter(brightness=(0.80, 1.20), contrast=(0.80, 1.20), saturation=0, hue=0),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Function to load, transform, and return a PIL image\n",
        "def load_and_transform_image(image_path):\n",
        "    # Load the image\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    # Apply the transform\n",
        "    transformed_image = transform(image)\n",
        "\n",
        "    # Convert the tensor back to a PIL image\n",
        "    pil_image = transforms.ToPILImage()(transformed_image)\n",
        "\n",
        "    return pil_image\n",
        "\n",
        "# Plot a 7x7 grid of images and save it as a single image\n",
        "def plot_image_grid(image_dir, save_path, num_images=49):\n",
        "    # Grid dimensions\n",
        "    grid_size = 7\n",
        "    image_size = 224  # Size of each image (224x224 after resizing)\n",
        "\n",
        "    # Create a blank canvas for the 7x7 grid\n",
        "    grid_image = Image.new('RGB', (grid_size * image_size, grid_size * image_size))\n",
        "\n",
        "    # Get the list of .jpg image files\n",
        "    image_files = [f for f in os.listdir(image_dir) if f.endswith('.jpg')][:num_images]  # Filter only .jpg files\n",
        "\n",
        "    for i, image_file in enumerate(image_files):\n",
        "        # Load and transform the image\n",
        "        image_path = os.path.join(image_dir, image_file)\n",
        "        transformed_image = load_and_transform_image(image_path)\n",
        "\n",
        "        # Calculate the position in the grid (row, col)\n",
        "        row = i // grid_size\n",
        "        col = i % grid_size\n",
        "\n",
        "        # Paste the transformed image into the grid at the calculated position\n",
        "        grid_image.paste(transformed_image, (col * image_size, row * image_size))\n",
        "\n",
        "    # Save the grid image to the specified path\n",
        "    grid_image.save(save_path)\n",
        "    print(f\"Grid image saved at: {save_path}\")\n",
        "\n",
        "    # Display the grid of images\n",
        "    plt.imshow(grid_image)\n",
        "    plt.axis('off')  # Hide the axes for a cleaner look\n",
        "    plt.show()\n",
        "\n",
        "# Example: Directory containing your images and save path\n",
        "image_dir = '/content/anchor_final/'  # Update this with your actual directory path\n",
        "save_path = '/content/saved_grid_image.jpg'  # Path to save the grid image\n",
        "plot_image_grid(image_dir, save_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQFuMOu9us2D"
      },
      "source": [
        "#1) DATALOADER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvLwOeiAawsI"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_5luENdax5k"
      },
      "outputs": [],
      "source": [
        "#new\n",
        "import os\n",
        "import random\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "\n",
        "\n",
        "# Example of data augmentation and normalization transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    #transforms.RandomHorizontalFlip(p=0.5),  # Randomly flip image with a probability of 0.5\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Randomly adjust brightness, contrast, etc.\n",
        "    #transforms.RandomRotation(10),  # Randomly rotate image within 10 degrees\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Standard ImageNet normalization\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "class SiameseDataset(Dataset):\n",
        "    def __init__(self, anchor_dir, train_dir, transform=None):\n",
        "        self.anchor_dir = anchor_dir\n",
        "        self.train_dir = train_dir\n",
        "        self.transform = transform\n",
        "        self.anchor_images = []\n",
        "        self.positive_images = {}\n",
        "        self.valid_image_extensions = ('.jpg', '.jpeg', '.png')\n",
        "\n",
        "        # Load anchor images\n",
        "        for img_name in os.listdir(anchor_dir):\n",
        "            img_path = os.path.join(anchor_dir, img_name)\n",
        "            if os.path.isfile(img_path) and img_name.lower().endswith(self.valid_image_extensions):\n",
        "                try:\n",
        "                    with Image.open(img_path) as img:\n",
        "                        img.verify()  # Verify that this is an image file\n",
        "                    class_name = os.path.splitext(img_name)[0]\n",
        "                    self.anchor_images.append((class_name, img_path))\n",
        "                except (IOError, SyntaxError):\n",
        "                    print(f\"File {img_path} is not a valid image.\")\n",
        "\n",
        "        # Load positive images\n",
        "        for class_name in os.listdir(train_dir):\n",
        "            class_path = os.path.join(train_dir, class_name)\n",
        "            if os.path.isdir(class_path):\n",
        "                self.positive_images[class_name] = []\n",
        "                for img_name in os.listdir(class_path):\n",
        "                    img_path = os.path.join(class_path, img_name)\n",
        "                    if os.path.isfile(img_path) and img_name.lower().endswith(self.valid_image_extensions):\n",
        "                        try:\n",
        "                            with Image.open(img_path) as img:\n",
        "                                img.verify()  # Verify that this is an image file\n",
        "                            self.positive_images[class_name].append(img_path)\n",
        "                        except (IOError, SyntaxError):\n",
        "                            print(f\"File {img_path} is not a valid image.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.anchor_images)\n",
        "\n",
        "    def load_image(self, img_path):\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        return img\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        anchor_class, anchor_path = self.anchor_images[index]\n",
        "\n",
        "        # Load anchor image\n",
        "        anchor_img = self.load_image(anchor_path)\n",
        "        if self.transform:\n",
        "            anchor_img = self.transform(anchor_img)\n",
        "\n",
        "        # Get a positive image\n",
        "        positive_path = random.choice(self.positive_images[anchor_class])\n",
        "        positive_img = self.load_image(positive_path)\n",
        "        if self.transform:\n",
        "            positive_img = self.transform(positive_img)\n",
        "\n",
        "        # Get a negative image (from a different class)\n",
        "        negative_class = random.choice(list(self.positive_images.keys()))\n",
        "        while negative_class == anchor_class:\n",
        "            negative_class = random.choice(list(self.positive_images.keys()))\n",
        "        negative_img = self.load_image(random.choice(self.positive_images[negative_class]))\n",
        "        if self.transform:\n",
        "            negative_img = self.transform(negative_img)\n",
        "\n",
        "        return anchor_img, positive_img, negative_img\n",
        "\n",
        "anchor_dir = \"/content/anchor_final\"\n",
        "train_dir = \"/content/train/\"\n",
        "\n",
        "dataset = SiameseDataset(anchor_dir=anchor_dir, train_dir=train_dir, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KV2sev6YGVUE"
      },
      "source": [
        "#2) Network Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qurm_bHeUZvE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "# Load the pretrained ResNet-18 model\n",
        "resnet18 = models.resnet18(pretrained=True)\n",
        "\n",
        "# Freeze layers\n",
        "'''\n",
        "for name, param in resnet18.named_parameters():\n",
        "    if 'layer4.1' not in name:  # Freeze up to layer3\n",
        "        param.requires_grad = False\n",
        "\n",
        "for param in resnet18.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "'''\n",
        "freeze_up_to_idx = 57\n",
        "\n",
        "# Freeze layers up to freeze_up_to_idx\n",
        "for idx, (name, param) in enumerate(resnet18.named_parameters()):\n",
        "    if idx < freeze_up_to_idx:\n",
        "        param.requires_grad = False\n",
        "    else:\n",
        "        param.requires_grad = True  # Unfreeze layers beyond freeze_up_to_idx\n",
        "\n",
        "# Modify the fully connected layer to output 128 dimension embedding\n",
        "num_ftrs = resnet18.fc.in_features\n",
        "resnet18.fc = nn.Linear(num_ftrs, 128)\n",
        "\n",
        "\n",
        "\n",
        "# Define the Siamese Network\n",
        "class SiameseNetwork(nn.Module):\n",
        "    def __init__(self, base_model):\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "        self.base_model = base_model\n",
        "\n",
        "    def forward_one(self, x):\n",
        "        return self.base_model(x)\n",
        "\n",
        "    def forward(self, input1, input2=None):\n",
        "        if input2 is None:\n",
        "            # Handle the case where only one input is provided\n",
        "            embedding = self.forward_one(input1)\n",
        "            return F.normalize(embedding, p=2, dim=1)  # Normalize the embedding\n",
        "        else:\n",
        "            # Handle the case where two inputs are provided\n",
        "            embedding1 = self.forward_one(input1)\n",
        "            embedding2 = self.forward_one(input2)\n",
        "            return F.normalize(embedding1, p=2, dim=1), F.normalize(embedding2, p=2, dim=1)\n",
        "\n",
        "\n",
        "# Initialize the Siamese Network with the modified ResNet-50\n",
        "model = SiameseNetwork(resnet18)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "bGzqlTBCYxWA"
      },
      "outputs": [],
      "source": [
        "# Count and print the number of frozen and trainable parameters\n",
        "num_frozen_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
        "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Number of frozen parameters: {num_frozen_params}\")\n",
        "print(f\"Number of trainable parameters: {num_trainable_params}\")\n",
        "\n",
        "# Print the modified ResNet-18 architecture with information about frozen and trainable layers\n",
        "for name, param in model.named_parameters():\n",
        "    status = \"Trainable\" if param.requires_grad else \"Frozen\"\n",
        "    print(f\"Layer: {name}, Status: {status}, Shape: {param.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "l6ZWPwieV-JA"
      },
      "outputs": [],
      "source": [
        "print(\" ResNet-18 architecture:\\n\", resnet18)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tGdwjgqBmjVZ"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "summary(model, [(3, 224, 224),(3,224,224)])  # Assuming input size is 224x224 and 3 channels (RGB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3XwsYBDK_3X"
      },
      "source": [
        "#3) Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYRlYT0BlT7W"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def triplet_loss(anchor, positive, negative, margin=0.8):\n",
        "    # Compute distances\n",
        "    d_p = F.pairwise_distance(anchor, positive, p=2)\n",
        "    print(f'd_p: {d_p}')\n",
        "    d_n = F.pairwise_distance(anchor, negative, p=2)\n",
        "    print(f'd_n: {d_n}')\n",
        "    # Compute loss\n",
        "    loss = torch.clamp(d_p - d_n + margin, min=0.0).mean()\n",
        "\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peRtN0-Dwpnb"
      },
      "source": [
        "#3.2) TRAINING LOOP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "cP754BKAmvUA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 100\n",
        "\n",
        "checkpoint_path = '/content/resnet18_checkpoint_RUN10.pth'  # Path to save/load checkpoint\n",
        "\n",
        "if os.path.exists(checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    start_epoch = checkpoint['epoch']\n",
        "    loss = checkpoint['loss']\n",
        "    print(f\"Loaded checkpoint '{checkpoint_path}' (epoch {start_epoch})\")\n",
        "else:\n",
        "    start_epoch = 0\n",
        "\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for i, (anchor_img, positive_img, negative_img) in enumerate(dataloader):\n",
        "        anchor_img = anchor_img.to(device)\n",
        "        positive_img = positive_img.to(device)\n",
        "        negative_img = negative_img.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        anchor_out, positive_out = model(anchor_img, positive_img)\n",
        "        negative_out = model(negative_img)\n",
        "        # Compute loss\n",
        "        loss = triplet_loss(anchor_out, positive_out, negative_out)\n",
        "        print(loss.item())\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if (i + 1) % 3 == 0:    # Print every 3 mini-batches\n",
        "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(dataloader)}], Loss: {running_loss / 3:.4f}', flush=True)\n",
        "            running_loss = 0.0\n",
        "\n",
        "    # Print average loss for the epoch\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Average Epoch Loss: {epoch_loss / len(dataloader):.4f}')\n",
        "    # Save checkpoint after 50 epochs\n",
        "    if epoch >= 49:\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "        }, checkpoint_path)\n",
        "\n",
        "print(\"Finished Training\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1HXkVfHasdg"
      },
      "source": [
        "#4) Load Model After Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkfzf9iWa38a"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# Define the model again (same as when you trained it)\n",
        "resnet18 = models.resnet18(pretrained=False)  # Do not load pretrained weights here\n",
        "num_ftrs = resnet18.fc.in_features\n",
        "resnet18.fc = nn.Linear(num_ftrs, 128)  # Modified final layer\n",
        "\n",
        "# Initialize the Siamese network\n",
        "model = SiameseNetwork(base_model=resnet18)\n",
        "\n",
        "# Load the checkpoint (replace 'path/to/checkpoint.pth' with your actual path)\n",
        "checkpoint = torch.load('/content/resnet18_checkpoint_RUN10.pth', map_location=device)  # Use 'cuda' if you have GPU\n",
        "\n",
        "# Load the model state from the checkpoint\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Example transform, similar to training setup\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Function to load and preprocess an image\n",
        "def load_image(image_path):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
        "    return image\n",
        "\n",
        "# Load the images you want to compute embeddings for\n",
        "image1 = load_image('/content/anchor_images/4133_p_g_1281.jpg')\n",
        "image2 = load_image('/content/test/4133_p_g_1281/4133_p_g_1281_11.jpg')\n",
        "\n",
        "# Inference with SiameseNetwork model to compute embeddings\n",
        "with torch.no_grad():\n",
        "    embedding1, embedding2 = model(image1, image2)\n",
        "\n",
        "# Check the embeddings\n",
        "#print(f\"Embedding 1: {embedding1}\")\n",
        "#print(f\"Embedding 2: {embedding2}\")\n",
        "\n",
        "# Calculate cosine similarity\n",
        "def cosine_similarity(embedding1, embedding2):\n",
        "    # Compute cosine similarity\n",
        "    cos_sim = F.cosine_similarity(embedding1, embedding2)\n",
        "    return cos_sim.item()  # Convert to a scalar\n",
        "\n",
        "# Call the cosine similarity function\n",
        "similarity = cosine_similarity(embedding1, embedding2)\n",
        "\n",
        "# Print the cosine similarity\n",
        "print(f\"Cosine Similarity: {similarity:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyFUM0doRA_k"
      },
      "source": [
        "#5) VALIDATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkU7STDnRDnL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define the model again\n",
        "resnet18 = models.resnet18(pretrained=False)  # Do not load pretrained weights here\n",
        "num_ftrs = resnet18.fc.in_features\n",
        "resnet18.fc = torch.nn.Linear(num_ftrs, 128)  # Modified final layer\n",
        "\n",
        "# Initialize the Siamese network\n",
        "model = SiameseNetwork(base_model=resnet18)\n",
        "\n",
        "# Load the checkpoint (replace 'path/to/checkpoint.pth' with your actual path)\n",
        "checkpoint = torch.load('/content/resnet18_checkpoint_RUN10.pth', map_location=device)\n",
        "\n",
        "# Load the model state from the checkpoint\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# Move the model to GPU if available\n",
        "model = model.to(device)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Define transformations for image preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Function to load and preprocess an image\n",
        "def load_image(image_path):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image = transform(image).unsqueeze(0).to(device)  # Add batch dimension and move to GPU\n",
        "    return image\n",
        "\n",
        "# Save embeddings for all anchor images\n",
        "def save_anchor_embeddings(anchor_folder_path):\n",
        "    embeddings = []  # To store the embeddings and filenames\n",
        "    filenames = []   # To store the corresponding filenames\n",
        "\n",
        "    # Iterate through all image files in the anchor folder\n",
        "    image_files = [f for f in os.listdir(anchor_folder_path) if f.endswith(('jpg', 'png'))]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for image_file in image_files:\n",
        "            image_path = os.path.join(anchor_folder_path, image_file)\n",
        "            try:\n",
        "                image = load_image(image_path)\n",
        "                embedding = model(image)\n",
        "\n",
        "                # Save embedding as a numpy array\n",
        "                embeddings.append(embedding.squeeze(0).cpu().numpy())\n",
        "\n",
        "                # Save the image path\n",
        "                filenames.append(image_path)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {image_path}: {e}\")\n",
        "\n",
        "    # Check if we have any embeddings before saving\n",
        "    if embeddings:\n",
        "        # Convert embeddings list to a numpy array\n",
        "        embeddings_np = np.vstack(embeddings)  # Shape (num_images, 128)\n",
        "\n",
        "        # Save embeddings and filenames to files\n",
        "        np.save('embeddings.npy', embeddings_np)\n",
        "        with open('filenames.txt', 'w') as f:\n",
        "            for filename in filenames:\n",
        "                f.write(f\"{filename}\\n\")\n",
        "\n",
        "        print(f\"Saved {len(embeddings)} embeddings and filenames.\")\n",
        "    else:\n",
        "        print(\"No embeddings to save.\")\n",
        "\n",
        "# Call the function to save embeddings of anchor images\n",
        "anchor_folder_path = '/content/anchor_images'  # Path to the anchor images folder\n",
        "save_anchor_embeddings(anchor_folder_path)\n",
        "\n",
        "def get_base_name(file_name, is_anchor=False):\n",
        "\n",
        "    if is_anchor:\n",
        "        # For anchor images: strip the file extension\n",
        "        return os.path.splitext(file_name)[0]\n",
        "    else:\n",
        "        # For query images: return the part before the last underscore\n",
        "        return '_'.join(file_name.split('_')[:-1])\n",
        "\n",
        "# Function to check if the anchor image matches the query image\n",
        "def check_match(anchor_filename, query_filename):\n",
        "    query_base = get_base_name(query_filename,is_anchor=False)\n",
        "    anchor_base= get_base_name(anchor_filename, is_anchor=True )\n",
        "    return anchor_base == query_base\n",
        "\n",
        "# Function to find the closest K images given a query image and check if anchor image is in top K\n",
        "def evaluate_top_k(query_image_path, top_k=3):\n",
        "    # Load the query image and compute its embedding\n",
        "    query_image = load_image(query_image_path)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        query_embedding = model(query_image).squeeze(0).to(device)  # Query embedding on GPU\n",
        "\n",
        "    # Load saved embeddings and move them to the GPU\n",
        "    saved_embeddings = torch.tensor(np.load('embeddings.npy')).to(device)\n",
        "\n",
        "    with open('filenames.txt', 'r') as f:\n",
        "        saved_filenames = f.readlines()\n",
        "\n",
        "    # Compute cosine similarity between query embedding and all saved embeddings\n",
        "    similarities = F.cosine_similarity(query_embedding, saved_embeddings, dim=-1)\n",
        "\n",
        "    # Get indices of top_k most similar images\n",
        "    top_k_indices = torch.topk(similarities, top_k).indices.tolist()\n",
        "\n",
        "    # Check if the anchor image is in the top K similar images\n",
        "    correct_anchor = False\n",
        "    for idx in top_k_indices:\n",
        "        anchor_filename = os.path.basename(saved_filenames[idx].strip())\n",
        "        query_filename = os.path.basename(query_image_path)\n",
        "\n",
        "        if check_match(anchor_filename, query_filename):\n",
        "            correct_anchor = True\n",
        "            break\n",
        "\n",
        "    return correct_anchor\n",
        "\n",
        "# Iterate through all query images in the test folder\n",
        "def evaluate_all_queries(test_folder_path, top_k=3):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Iterate through all subfolders and query images\n",
        "    for root, dirs, files in os.walk(test_folder_path):\n",
        "        for file in files:\n",
        "            if file.endswith(('jpg', 'png')):\n",
        "                query_image_path = os.path.join(root, file)\n",
        "                total += 1\n",
        "\n",
        "                # Evaluate the top K results for the current query image\n",
        "                if evaluate_top_k(query_image_path, top_k=top_k):\n",
        "                    correct += 1\n",
        "                else:\n",
        "                    print(f\"Wrong for query: {query_image_path}\")\n",
        "\n",
        "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
        "    print(f\"Accuracy: {accuracy:.2f}% ({correct}/{total} correct)\")\n",
        "\n",
        "# Example usage: Evaluate all queries in the test folder\n",
        "test_folder_path = '/content/test/'  # Path to the test folder with subfolders\n",
        "evaluate_all_queries(test_folder_path, top_k=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "c1ibU1B52-Ia"
      },
      "outputs": [],
      "source": [
        "# Function to find the closest K images given a query image and check if anchor image is in top K\n",
        "def evaluate_top_k(query_image_path, top_k=3):\n",
        "    # Load the query image and compute its embedding\n",
        "    query_image = load_image(query_image_path)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        query_embedding = model(query_image).squeeze(0).to(device)  # Query embedding on GPU\n",
        "\n",
        "    # Load saved embeddings and move them to the GPU\n",
        "    saved_embeddings = torch.tensor(np.load('embeddings.npy')).to(device)\n",
        "\n",
        "    with open('filenames.txt', 'r') as f:\n",
        "        saved_filenames = f.readlines()\n",
        "\n",
        "    # Compute cosine similarity between query embedding and all saved embeddings\n",
        "    similarities = F.cosine_similarity(query_embedding, saved_embeddings, dim=-1)\n",
        "\n",
        "    # Get indices of top_k most similar images\n",
        "    top_k_indices = torch.topk(similarities, top_k).indices.tolist()\n",
        "\n",
        "    # Print the query image name\n",
        "    print(f\"\\nQuery Image: {os.path.basename(query_image_path)}\")\n",
        "\n",
        "    # Store the information of top K similar anchor images\n",
        "    top_k_anchors = []\n",
        "    for idx in top_k_indices:\n",
        "        anchor_filename = os.path.basename(saved_filenames[idx].strip())\n",
        "        similarity = similarities[idx].item()\n",
        "        top_k_anchors.append((anchor_filename, similarity))\n",
        "        print(f\"Top {len(top_k_anchors)} Similar Anchor: {anchor_filename}, Similarity: {similarity:.4f}\")\n",
        "\n",
        "    # Check if the correct anchor image is in the top K similar images\n",
        "    correct_anchor = False\n",
        "    for anchor_filename, _ in top_k_anchors:\n",
        "        if check_match(anchor_filename, os.path.basename(query_image_path)):\n",
        "            correct_anchor = True\n",
        "            break\n",
        "\n",
        "    return correct_anchor\n",
        "\n",
        "# Iterate through all query images in the test folder\n",
        "def evaluate_all_queries(test_folder_path, top_k=3):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Iterate through all subfolders and query images\n",
        "    for root, dirs, files in os.walk(test_folder_path):\n",
        "        for file in files:\n",
        "            if file.endswith(('jpg', 'png')):\n",
        "                query_image_path = os.path.join(root, file)\n",
        "                total += 1\n",
        "\n",
        "                # Evaluate the top K results for the current query image\n",
        "                if evaluate_top_k(query_image_path, top_k=top_k):\n",
        "                    correct += 1\n",
        "                else:\n",
        "                    print(f\"Wrong for query: {query_image_path}\")\n",
        "\n",
        "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
        "    print(f\"\\nAccuracy: {accuracy:.2f}% ({correct}/{total} correct)\")\n",
        "\n",
        "# Example usage: Evaluate all queries in the test folder\n",
        "test_folder_path = '/content/test/'  # Path to the test folder with subfolders\n",
        "evaluate_all_queries(test_folder_path, top_k=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cz63r_BN5y1D",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to plot the query image and top K similar anchor images\n",
        "def plot_query_and_top_k(query_image_path, top_k_anchors):\n",
        "    fig, axes = plt.subplots(1, len(top_k_anchors) + 1, figsize=(15, 5))\n",
        "\n",
        "    # Plot the query image\n",
        "    query_image = Image.open(query_image_path).convert('RGB')\n",
        "    axes[0].imshow(query_image)\n",
        "    axes[0].set_title(f\"Query Image\")\n",
        "    axes[0].axis('off')  # Hide axes\n",
        "\n",
        "    # Plot the top K anchor images\n",
        "    for i, (anchor_filename, similarity) in enumerate(top_k_anchors):\n",
        "        anchor_image = Image.open(anchor_filename).convert('RGB')\n",
        "        axes[i + 1].imshow(anchor_image)\n",
        "        axes[i + 1].set_title(f\"Anchor {i+1}\\nSim: {similarity:.4f}\")\n",
        "        axes[i + 1].axis('off')  # Hide axes\n",
        "\n",
        "    # Display the plot\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Function to find the closest K images given a query image and check if anchor image is in top K\n",
        "def evaluate_top_k(query_image_path, top_k=3):\n",
        "    # Load the query image and compute its embedding\n",
        "    query_image = load_image(query_image_path)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        query_embedding = model(query_image).squeeze(0).to(device)  # Query embedding on GPU\n",
        "\n",
        "    # Load saved embeddings and move them to the GPU\n",
        "    saved_embeddings = torch.tensor(np.load('embeddings.npy')).to(device)\n",
        "\n",
        "    with open('filenames.txt', 'r') as f:\n",
        "        saved_filenames = f.readlines()\n",
        "\n",
        "    # Compute cosine similarity between query embedding and all saved embeddings\n",
        "    similarities = F.cosine_similarity(query_embedding, saved_embeddings, dim=-1)\n",
        "\n",
        "    # Get indices of top_k most similar images\n",
        "    top_k_indices = torch.topk(similarities, top_k).indices.tolist()\n",
        "\n",
        "    # Print the query image name\n",
        "    print(f\"\\nQuery Image: {os.path.basename(query_image_path)}\")\n",
        "\n",
        "    # Store the information of top K similar anchor images\n",
        "    top_k_anchors = []\n",
        "    for idx in top_k_indices:\n",
        "        anchor_filename = saved_filenames[idx].strip()\n",
        "        similarity = similarities[idx].item()\n",
        "        top_k_anchors.append((anchor_filename, similarity))\n",
        "        print(f\"Top {len(top_k_anchors)} Similar Anchor: {os.path.basename(anchor_filename)}, Similarity: {similarity:.4f}\")\n",
        "\n",
        "    # Plot the query image and top K anchor images\n",
        "    plot_query_and_top_k(query_image_path, top_k_anchors)\n",
        "\n",
        "    # Check if the correct anchor image is in the top K similar images\n",
        "    correct_anchor = False\n",
        "    for anchor_filename, _ in top_k_anchors:\n",
        "        if check_match(os.path.basename(anchor_filename), os.path.basename(query_image_path)):\n",
        "            correct_anchor = True\n",
        "            break\n",
        "\n",
        "    return correct_anchor\n",
        "\n",
        "# Iterate through all query images in the test folder\n",
        "def evaluate_all_queries(test_folder_path, top_k=3):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Iterate through all subfolders and query images\n",
        "    for root, dirs, files in os.walk(test_folder_path):\n",
        "        for file in files:\n",
        "            if file.endswith(('jpg', 'png')):\n",
        "                query_image_path = os.path.join(root, file)\n",
        "                total += 1\n",
        "\n",
        "                # Evaluate the top K results for the current query image\n",
        "                if evaluate_top_k(query_image_path, top_k=top_k):\n",
        "                    correct += 1\n",
        "                else:\n",
        "                    print(f\"Wrong for query: {query_image_path}\")\n",
        "\n",
        "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
        "    print(f\"\\nAccuracy: {accuracy:.2f}% ({correct}/{total} correct)\")\n",
        "\n",
        "# Example usage: Evaluate all queries in the test folder\n",
        "test_folder_path = '/content/test/'  # Path to the test folder with subfolders\n",
        "evaluate_all_queries(test_folder_path, top_k=3)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}